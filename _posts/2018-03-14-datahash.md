---
title: 浅谈分布式存储系统数据分布方法
tags: [读后感, 分布式存储]
---

## 指标

数据分布算法基本指标: 均匀性, 稳定性. 参见原文了解这两个指标具体的细节, 以及相互之间的矛盾.

数据分布算法其他指标, 参见原文. 节点异构目测是指节点具有不一样的结构与性能, 即具有不同的存储容量以及读写速度.

数据分布算法需要为每一个给定的 key 指定一个或多个对应的存储节点负责. 根据原文 "隔离故障域" 来看, 将一个 key 映射为一组存储节点, 这组节点存放着 key 以及 key 的备份; 按我理解, 副本分布这种事不应该是数据分布算法要考虑的, 数据分布算法只负责将 key 映射到一个节点, 该节点是 master 节点负责 key 的读写操作, 至于 master 节点与哪些节点在一起组成一个存储节点组是其他模块负责的事情.

隔离故障域, 按我理解这些不应该是分布算法负责的. 以三副本为例, 分布算法只负责得到一个节点, 由元信息服务存放该节点所处的副本节点组, 并且元信息服务在确认节点组时已经考虑了节点所处机架等物理拓扑信息.

## 演进

这里假设 key 的值足够分散. 按我理解应该是指用户的 key 均匀分散在整个 key 空间; 毕竟如果用户 key 挤在一坨, 再好的数据分布算法也无能无力吧.

### Hash

参见原文. 很简单直观的数据分布算法.

### 一致性 Hash, 带虚拟节点的一致性 Hash

参见原文相应章节了解这两个数据分布算法. 由于没见过真正一致性 hash, 带虚拟节点的一致性 hash 实现, 所以不是很了解具体细节.

以三个节点 A, B, C 组成的一致性 hash 为例, 此时假设圆心为 O, 角 AOB = x, 角 BOC = y, 角 COA = z; 很显然 x + y + z = 360 度; 当 A, B, C 节点容量与性能一致时, x = y = z = 120 度. 当 A, B 节点容量是 C 的两倍时, 此时 z = 2 * y, x = 2 * y, 求得 y = 72, x = z = 144. 也就是可以根据 A, B, C 的性能来调整 x, y, z 的取值. 也即一致性哈希可以处理节点异构的情况.

### 带负载上限的一致性 Hash

参见原文, 没咋地看懂, 有空看一下原论文. 另外我觉得该篇论文是 google 2017 年发表的, 所以不至于留下 "不能处理节点异构" 这么个烂摊子, 那么这里所说的节点负载是不是用来处理节点异构的一种权重呢? 容量大性能优越的节点的节点负载倍数更高之类的.

### 分片

参见原文, 分片还是比较直观易懂的.

"分片跟上面提到的虚拟节点有着很本质的区别，分片的划分和分片的分配被解耦". 按我理解虚拟节点与分片本质上是一样的, 虚拟节点的划分与分配也不是连在一块的啊==!

由于分片数相对 key 空间已经很小并且数量确定, 可以更精确地初始设置; 按我理解是分片数较少, 分片到实际节点的映射可以由单机存储, 不需要再次划分了; 如果需要再次划分, 那就涉及到递归了, 为了解决 key 切分问题引入了分片, 然后由引入了分片的切分问题...

Zeppelin 中的分片方式; 这个和我想的不太一样, 我想的是, 一个分片对应着一个存储节点组的 master 节点, 由该 Master 节点负责分片的读写以及数据的同步, 也即分片本身并不关心其对应的存储节点组副本的个数; 但是再 zeppelin 的做法中, 貌似每一个分片的副本数 3 是固定的.


### CRUSH算法

参见原文, 没咋地懂.

## 应用

参见原文, 目测还是用分片机制的多一点.


## 参考

参见原文, 记录了一些论文原文, 可以用来做延伸阅读.


## 参考

-   [浅谈分布式存储系统数据分布方法][20180314125940]

[20180314125940]: <https://mp.weixin.qq.com/s/QIeQALt9qK6YJ8ijGr9FgQ> "版本: 2017-12-20"

