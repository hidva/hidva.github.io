---
title: "GP 中的事务"
hidden: true
tags: ["Postgresql/Greenplum"]
---


分布式事务 ID, 有两部分组成: timestamp, dxid. 其中 timestamp 为实例启动时的时间戳, 在 tmShmemInit() 中被初始化为 `time()` 返回值, 之后在实例运行期间一直保持不变. dxid 是一个简单地计数器, 其在实例启动时在 tmShmemInit() 中被初始化为 1; 函数 currentDtxActivate() 会负责为当前分布式事务分配一个 dxid. 目前看仅当 query 需要 write 时才会分配 dxid, 以及显式 BEGIN 开启事务时也会分配. 当 dxid 到达 0xffffffff 时, 表明在当前实例生命周期内无法再分配 dxid 了, 因此系统会 PANIC 重启.

分布式事务 ID 的比较. 很简单, 先比较 timestamp, 再比较 dxid 即可. 但一般情况下比较中的两个分布式事务, 总是有一个事务是在实例当前生命周期分配的, 即 timestamp 与实例 tmShmemInit() 初始化值一致. 此时若另外一个分布式事务 B.timestamp 与该事物 A 不一致, 则表明另外一个是事务是在实例之前生命周期分配的, 所以 B 会被认为早于 A.

分布式事务快照. GP 中针对 distributed transaction id, distributed snapshot 的实现基本上就是把 PG 中 xid, snapshot 的实现照搬了一遍, 只不过是把事务的概念扩展到分布式事务. 关于 PG 中 xid, snapshot 的介绍可参考 [PG 中的事务: 快照]({{site.url}}/2019/12/01/pgxactsnapshot/). 这篇文章中所用到的设施都可以在 GP 中找到对等的实现, 比如与 xid 对应的是 distributed transaction id, 即 gxid. 与 `ShmemVariableCache->latestCompletedXid` 对应的是 `ShmemVariableCache->latestCompletedDXid`, 与 XidGenLock 对应的是 shmGxidGenLock 这个 lock, 与 PGXACT 对应的结构是 TMGXACT 等等.

TMGXACT; 这个结构起到了 PG 中 PGXACT 的作用, 其内存放着当前 backend 分布式事务相关的一些信息.

DistributedSnapshot; GP 中使用 DistributedSnapshot 表示正在运行着的分布式事务. 此时 distribTransactionTimeStamp 表明这些分布式事务 timestamp 部分的值. 因为这些运行中的分布式事务都是在实例当前生命周期内分配的, 因此他们具有相同的 timestamp 取值. distribSnapshotId 为分布式快照 id. 下一个待分配的快照 id 存放在 shmNextSnapshotId 中, 其指向的空间由 tmShmemInit() 负责分配并初始化为 0. xminAllDistributedSnapshots 对应着 PG 中 RecentGlobalXmin 取值, 相当于是所有连接 TMGXACT::xminDistributedSnapshot 的最小值. 剩下的 xmin, xmax, inProgressXidArray 就类似于 PG SnapShotData 中相应成员, 只不过是分布式事务 dxid.

CreateDistributedSnapshot() 根据当前情况构造一个分布式快照. 类似于 PG 的 GetSnapshotData. 分布式事务 dxid 在函数 currentDtxActivate 中分配. 这里使用 shmGxidGenLock 来进行同步, 该 lock 类似于 PG 的 XidGenLock.

GP 分布式事务的大致实现. 每次用户在 GP 中开启一个 distributed transaction 时, 位于每个 segment 上归属于当前 session 的 segmates 中的 writer backend 就会开启一个 local transaction, 之后 distributed transaction 的所有行为都发生在每个 segment 的 local transaction 中. 当 distributed transaction 被提交时, master 上的 QD 就会下发 PREPARE TRANSACTION 命令给每一个 local transaction, 在所有 local transaction 正确地完成 PREPARE 之后, QD 接着会下发 COMMIT PREPARED 命令给所有 local transaction 来完成每个 local transaction 的提交. 当所有 local transaction 提交完毕之后, 整个分布式事务也便认为已经提交完成了. COMMIT PREPARED 理论上总应该是成功的, 若真的不幸有某个 segment 上 COMMIT PREPARED 失败, 那么 GP 就会反复重试该操作. GP 这里并未像 PG clog 那样维护着每个事务的提交状态, 因为 global transaction 的提交状态就等同于其 local transaction 的提交状态, 即对于分布式事务 A, 若我们发现其在某个 segment 上对应局部事务 B 是 commited, 那么 A 在其他 segment 上对应的局部事务也总是提交的.

RecentGlobalXmin. 考虑到分布式事务的存在, GP 中的 segment 的 RecentGlobalXmin 并不能直接是原 PG GetSnapshotData 逻辑计算的结果. 考虑分布式事务 A 在三个 segment 各对应着 local transaction lx1, lx2, lx3. 当 A 提交时, 其会对三个 segment 下发 COMMIT PREPARED 提交 lx1,lx2,lx3. 但三个 segment 并不可能同一时间完成三个事务的提交, 可能会出现 lx1, lx3 提交了, lx2 正在提交中. 此时在 lx1 segment 上计算 RecentGlobalXmin 将会是 latestCompletedXid + 1, 即 lx1 + 1. 但此时 lx1 对应的分布式事务还没有提交呢, 所以这里计算出来的 global xmin 有点大了. GP 中通过 DistributedLogShared->oldestXmin 来维护着每个 segment db global xmin 的取值. oldestXmin 要满足语义: 小于 oldestXmin 的 local transaction 对应的 global transaction 都已经结束. 在实例启动时, 其会被初始化, 具体初始化逻辑还未看, 我理解应该会初始化为下一个 xid. 之后 GP 会在 GetSnapshotData() 调用时通过函数 DistributedLog_AdvanceOldestXmin() 来前移 oldestXmin. 在 DistributedLog_AdvanceOldestXmin 调用参数中: oldestLocalXmin 为 PG GetSnapshotData 根据 local transaction 信息计算出来的 RecentGlobalXmin, distribTransactionTimeStamp,xminAllDistributedSnapshots 是分布式事务下 global xmin, 即任何在其之前发生的分布式事务都已经结束了. DistributedLog_AdvanceOldestXmin 的逻辑很简单, 其会从  DistributedLogShared->oldestXmin 遍历到 oldestLocalXmin, 对于这里面每个 xid 都获取到对应的 global dxid, 若此时 global dxid < {distribTransactionTimeStamp,xminAllDistributedSnapshots}, 则会前移  DistributedLogShared->oldestXmin, 直至到达 oldestLocalXmin. 在实现上 DistributedLog_AdvanceOldestXmin 可能会被同一个 segment 属于不同 segmate 的 writer 并发调用. 我理解这时安全的操作. 另外 DistributedLog_AdvanceOldestXmin 中使用 TransactionIdPrecedes 来比较 dxid 是一个 bug. 参见 [PR](https://github.com/greenplum-db/gpdb/pull/9723).

DistributedLog. GP 通过该模块存放着每个 local xid 对应的 distribute xid. 该模块采用了类似 clog 的机制来管理, 其也使用了 SLRU 实现 cache. 该模块中每个 local xid 对应着 8bytes, 存放着 local xid 对应的 global transaction id. 仅当 local xid 被提交时, 即 COMMIT PREPARED 时, GP 才会将 local xid 以及对应的 global transaction id 通过函数 DistributedLog_SetCommittedWithinAPage() 写入.

distributed log 会被周期性地截断, 以节省存储空间. 函数 DistributedLog_AdvanceOldestXmin() 负责完成这一过程. 这里截断操作是安全的, 不会导致任何有损数据安全性的风险. 因为 DistributedLog_AdvanceOldestXmin() 内总是截断 DistributedLogShared->oldestXmin 之前的 local transaction 对应的 distributed log 信息. 这些 local transaction 对应的分布式事务总是已经结束了的, 也即针对这些分布式事务的提交状态, 我们只需要查看 local transaction 的提交状态即可. 也即我们再也不会关心这些 local transaction 对应分布式事务的相关信息了.

DistributedLog_CommittedCheck() 读取 DistributedLog 返回一个 local xid 对应着的 global transaction id. 这里之所以名字中包含 "committed check" 主要是因为只有被提交的 local xid 其对应的 global transaction id 才会被写入, 所以若能够从 DistributedLog 中读取到 local xid 对应的信息, 那边表明该 local xid 已经是提交了的. 若未能在 distributed log 找到对应信息, 则要么是这些 local transaction 提交了, 只不过对应的 distributed log 信息被 DistributedLog_AdvanceOldestXmin 截断了. 要么是因为这些 local transaction abort 了. 也即总是需要结合 clog 信息来做判定.

cdblocaldistribxact.c. 其内 cache 了 local xid 与其对应 dxid 的信息, 减少了对 DistributedLog 文件的访问. 其内使用 LRU 策略来管理 cache 的换入换出. 涉及到的结构有: LocalDistribCacheHtab, 类似于 `std::unordered_map<localxid, LocalDistribXactCacheEntry>`, 负责存放所有被 cache 的 local xid 以及 dxid 组合. LocalDistribXactCache, 扮演了 LRU cache 实现中双链表的角色, 其结构类似于 `std::list<LocalDistribXactCacheEntry>`, 位于链表头的 cache entry 总是最近一次被访问到的 entry. gp_max_local_distributed_cache 这个 GUC 用来指定 cache capacity, 若取值为 0, 则表明不启用 cache. 注意这里 cache entry 中并未存放 global transaction id 中 timestamp 的部分, 也就是意味着所有在 LocalDistribXactCache 中的分布式事务的 timestamp 总是等于当前实例启动时分配到的 timestamp. LocalDistribXactCache_CommittedFind() 用来查找该 cache. LocalDistribXactCache_AddCommitted() 用来往 cache 中新增一项. 可以根据这俩函数的实现来了解 cache 的详细结构.

DistributedSnapshotWithLocalMapping; 在 DistributedSnapshot 之上加了一些 cache 性的包装. 用来提升 XidInMVCCSnapshot() 函数的检测效率. 字段 inProgressMappedLocalXids 存放着 ds 内所有正在运行着的分布式事务在当前 segment 上对应的 local transaction xid. minCachedLocalXid, maxCachedLocalXid 则分别是 inProgressMappedLocalXids 中的最小值, 最大值. 所以在 XidInMVCCSnapshot 中若发现某个 xid 在 inProgressMappedLocalXids 存在, 那意味着该 xid 对应的分布式事务正在运行中, 省去了对 distributed log 与 DistributedSnapshot 的查找与检测.

DistributedSnapshotWithLocalMapping_CommittedTest(); 用来检测 localXid 对应的分布式事务是否在 DistributedSnapshot 中. 相当于是 PG XidInMVCCSnapshot() 的分布式版本. 返回:

-   DISTRIBUTEDSNAPSHOT_COMMITTED_INPROGRESS; 意味着在 DistributedLog 中找到了该 local xid 对应的分布式事务 id 信息, 并且该分布式事务在 DistributedSnapshot 中, 即该分布式事务正在运行中.

-   DISTRIBUTEDSNAPSHOT_COMMITTED_VISIBLE; 意味着在 DistributedLog 中找到了该 local xid 对应的分布式事务 id 信息, 并且该分布式事务不在 DistributedSnapshot 中, 即该分布式事务已经结束运行. 另外又由于只有当 local xid 提交时, 才会写入进 DistributedLog 中, 所以这时也意味着 local xid 已经提交了.

-   DISTRIBUTEDSNAPSHOT_COMMITTED_IGNORE; 意味着在 DistributedLog 中找到了该 local xid 对应的分布式事务 id 信息, 并且该分布式事务是在实例上一次生命周期中分配的. (我理解这时也应该返回 DISTRIBUTEDSNAPSHOT_COMMITTED_VISIBLE 的...

-   DISTRIBUTEDSNAPSHOT_COMMITTED_UNKNOWN; 意味着没有在 DistributedLog 中找到了该 local xid 对应的分布式事务 id 信息. (按我理解这是也可以设置 HEAP_XMIN_DISTRIBUTED_SNAPSHOT_IGNORE hint 了..

DISTRIBUTEDSNAPSHOT_COMMITTED_INPROGRESS/DISTRIBUTEDSNAPSHOT_COMMITTED_VISIBLE 分别对应着 XID_IN_SNAPSHOT/XID_NOT_IN_SNAPSHOT.

SnapshotData, GP 中 SnapshotData 存放着分布式快照 DistributedSnapshot, 以及 local snapshot. 这里 DistributedSnapshot 与 local snapshot 一一对应, 即对于每一个 DistributedSnapshot 中的分布式事务, 都有且仅有一个 local transaction 在 local snapshot 中与之对应. SnapshotData 中 DistributedSnapshot 存放在 SnapshotData::distribSnapshotWithLocalMapping::ds 中, 字段 haveDistribSnapshot 表明当前 SnapshotData 是否包含了分布式快照. GetSnapshotData() 用来填充 SnapshotData, 在不同角色下, 其具有不同的行为:

-   对于 QD, 此时需要完成 DistributedSnapshot, local snapshot 的获取. 对于 QE writer, 需要从 DtxContextInfo::distributedSnapshot 拷贝出 QD 发来的分布式快照, 然后再计算出 local snapshot, 并通过函数 updateSharedLocalSnapshot 填充到 SharedSnapshotSlot 中供  QE 使用. 对于 QE reader, 需要从 SharedSnapshotSlot 中获取由 writer 填充的分布式快照与 local snapshot 信息; 之后便直接返回了.
-   DistributedLogShared->oldestXmin 的更新.


GP 分布式事务可见性检测的简单描述: 在 segment 上的 local transaction 所持有的 snapshot 包括: 正在运行的 local transaction 集合, 正在运行着的 global transaction 集合, 这两个集合是等价的, 即这里 global transaction 与 local transaction 一一对应. 在扫描 tuple 时, 若一行 tuple 的 xmin 在正在运行着的 local transaction 集合中, 则表明该 tuple 所对应 local transaction, 所对应 global transaction 尚未提交, 此时该 tuple 不可见. 若当前 tuple 不在正在运行 local transaction 集合中, 那么此时会根据 distributedlog 模块找出 local transaction 对应的 global transaction, 若此时 global transaction 正在运行, 则 tuple 不可见. 若此时 global transaction 不在正在运行, 那么表明该 global transaction 已经结束, 此时再根据 clog 找到 local transaction 提交状态, 也就是 global transaction 提交状态. 也就对于一个 abort/commit 的 global transaction, 其下所有 local transaction 一定都是 abort/commit 的.

HeapTupleSatisfiesMVCC() 用来检测一个特定的 tuple 是否对一个特定的 snapshot 可见. 同时也会根据检查结果设置某些 hint 用来加速后续对同一 tuple 的检测. 比如若第一次检测某个 tuple 时, 发现其 xmin 已经提交了, 那么便会将 HEAP_XMIN_COMMITTED hint 保存在该 tuple 的 t_infomask 字段中, 参考 transam/README 'Writing Hints' 节介绍, 这种更改也会在可能的时候被持久化, 这样后续对同一 tuple 的检测就可以省略掉查询 clog 来判断 xmin 是否提交了. 在 PG 基础上, GP 也新增了部分 hint:

-   HEAP_XMIN_DISTRIBUTED_SNAPSHOT_IGNORE; 若 tuple t_infomask2 中存在该 flag, 则表明该行对应的 tuple 可见性检查时不需要再次检查 distributed snapshot. HeapTupleSatisfiesMVCC() 一般会在这些 tuple 所在 local transaction 对应的 distributed log 已经被 DistributedLog_AdvanceOldestXmin() 截断时为这些 tuple 设置 HEAP_XMIN_DISTRIBUTED_SNAPSHOT_IGNORE flag, 这样后续检测时就可以省略对分布式快照的检测了.


XidInMVCCSnapshot() 用来检查一个特定的 xid 是否在指定 snapshot 中. 参数 distributedSnapshotIgnore 指定了 tuple 中是否设置了 HEAP_XMIN_DISTRIBUTED_SNAPSHOT_IGNORE hint. 参数 setDistributedSnapshotIgnore 用来返回是否需要给 tuple 设置 HEAP_XMIN_DISTRIBUTED_SNAPSHOT_IGNORE hint. XidInMVCCSnapshot() 会返回 XID_IN_SNAPSHOT/XID_NOT_IN_SNAPSHOT 来表明查找结果. 返回 XID_SURELY_COMMITTED 表明当前 xid 不再 snapshot 中, 并且该 xid 已经得知是 commited 了, 不需要再次查找 clog 确认了. 比如当 DistributedSnapshotWithLocalMapping_CommittedTest 返回 DISTRIBUTEDSNAPSHOT_COMMITTED_VISIBLE 时, XidInMVCCSnapshot 便可以返回 XID_SURELY_COMMITTED 了.

## GlobalXmin 与 snapshot

GlobalXmin 主要用来控制 VACUUM 的行为. 当 VACUUM 发现某个 tuple 对应的 XMAX 已经提交, 也即该 tuple 已经被删除了. 并且该行 XMAX 先于 GlobalXmin, 即 TransactionIdPrecedes(XMAX, GlobalXmin) 返回 TRUE. 那么意味着当前 tuple 不会再被任何 backend 读取了, VACUUM 可以安全地删除这一 tuple 了.

为了维护计算 GlobalXmin, PG 引入了 PGXACT::xmin, 记录着当前 backend 内所有现存 snapshot 中 SnapshotData::xmin 的最小值. 所以这时 GlobalXmin 便是所有 backend 对应 PGXACT 结构中 xmin 的最小值了.

而为了维护更新 PGXACT::xmin, PG 引入了 snapmgr.c 模块, 用来追踪管理当前 backend 分配的所有 snapshot. backend 内分配的所有 snapshot 都会放入到 RegisteredSnapshots 中, RegisteredSnapshots 使用了 pairingheap 这一数据结构, 对 pairingheap 不了解的话可以暂且把他视为最小堆, 具有最小 xmin 的 snapshot 总是会被放在堆顶. PG 中 snapshot 可能会被 resowner.c, active snapshot stack 两个模块引用, SnapshotData::active_count, SnapshotData::regd_count 这两个计数器便是用来分别追踪当前 snapshot 被引用的次数, 仅当这两个引用次数都变为 0 时, snapshot 才能被释放. 存在一些特殊的 snapshot, 虽然他们引用计数不为 0, 但是他们并未被任何 resowner.c, active snapshot stack 引用着, 这些 snapshot 会被特殊处理, 具体哪些 snapshot 参考 snapmgr.c 头部注释. 简单来说有: FirstXactSnapshot, any snapshots that have been exported by pg_export_snapshot, CatalogSnapshot, historic snapshots used during logical decoding.

每当一个 snapshot 由于不再被其他模块引用而释放时, 该 snapshot 便会从 RegisteredSnapshots 中移除, 接着 PG 会根据变更后的 RegisteredSnapshots 结构判断是否可以前移 PGXACT::xmin. 简单来说若变更后 RegisteredSnapshots 堆顶 snapshot xmin 大于 PGXACT::xmin 时, PG 便会更新 PGXACT::xmin. 函数 SnapshotResetXmin() 实现了这一过程, 其会在需要时被 PG 调用. 另外为了简单实现, only registered snapshots not active snapshots participate in tracking which one is oldest; we don't try to change MyPgXact->xmin except when the active-snapshot stack is empty.

RegisteredSnapshots; 其结构类似于 `std::min_heap<SnapshotData*>`, 也即其中存放的只是 SnapshotData 地址, 当我们把一个 SnapshotData 从 RegisteredSnapshots 中移除时, SnapshotData 本身内存如何释放需要由上层调用着考虑, 可能会直接调用 pfree, 可能由于 SnapshotData 是 static-alloced 的, 而不采取任何动作.

active snapshot stack; ActiveSnapshot 总是指向着 active snapshot stack 栈顶元素. 按我理解, 当 PG 其他模块需要使用一个 snapshot 来判断可见性时, 他们总是使用 ActiveSnapshot 指向的快照. PushActiveSnapshot()/PopActiveSnapshot() 用来操作 active snapshot stack.